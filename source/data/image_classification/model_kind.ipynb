{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fb3d7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97480b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5963320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height, img_width = 224, 224  # 이미지 크기\n",
    "batch_size = 16  # batch size 너무 크게하면 메모리 초과 뜸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67265c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loc = 'C:/Users/multicampus/projects/fresh/data'  # your data loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1890852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 139067 files belonging to 18 classes.\n",
      "Using 111254 files for training.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    f'{data_loc}/Training/images',\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,  # random 값 고정\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fcc7d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 139067 files belonging to 18 classes.\n",
      "Using 27813 files for validation.\n"
     ]
    }
   ],
   "source": [
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  f'{data_loc}/Training/images',\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0d3b8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20098 files belonging to 18 classes.\n"
     ]
    }
   ],
   "source": [
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    f'{data_loc}/Validation/images',\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "430619e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = train_ds.class_names\n",
    "num_classes = len(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c38f2791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process(image,label):\n",
    "#     image = tf.cast(image/255. ,tf.float32)\n",
    "#     return image,label\n",
    "\n",
    "# train_ds = train_ds.map(process)\n",
    "# val_ds = val_ds.map(process)\n",
    "# test_ds = test_ds.map(process)                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bd9df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open('./my_model_class.txt', 'w')\n",
    "# f.write(str(train_ds.class_names))\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c25d8359",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "#   tf.keras.layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "  tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# tf.keras.layers.Input(shape=(None, None, 3)),\n",
    "# tf.keras.layers.Resizing(500, 500),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aecb3f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.Sequential([\n",
    "# #   tf.keras.layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "#   tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "#   tf.keras.layers.MaxPooling2D(),\n",
    "#   tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "#   tf.keras.layers.MaxPooling2D(),\n",
    "#   tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "#   tf.keras.layers.MaxPooling2D(),\n",
    "#   tf.keras.layers.Flatten(),\n",
    "#   tf.keras.layers.Dense(128, activation='relu'),\n",
    "#   tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da848db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c309817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e86fadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f39ea0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model_kind_1108.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe777c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model_kind_1109.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f99fe5",
   "metadata": {},
   "source": [
    "## mobile net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "735bd539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet import MobileNet, preprocess_input, decode_predictions\n",
    "from tensorflow.keras.preprocessing import image\n",
    "pre_trained_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62261b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in pre_trained_model.layers:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22f7ae2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 7, 7, 1024) dtype=float32 (created by layer 'conv_pw_13_relu')>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_trained_model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76632c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_layer(model, num_classes):\n",
    "    # 1 dimension의 output으로 flatten하기\n",
    "    last_output = model.output\n",
    "    x = tf.keras.layers.Flatten()(last_output)\n",
    "    # 1,024개의 hidden unit과 ReLU activation을 포함한 fully connected layer 추가\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    # classification을 위한 마지막 sigmoid layer 추가하기\n",
    "    x = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(model.input, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f01fb7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = add_layer(pre_trained_model, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2b232ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fe0cd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6954/6954 [==============================] - 1453s 208ms/step - loss: 0.2341 - accuracy: 0.9375 - val_loss: 0.1689 - val_accuracy: 0.9590\n",
      "Epoch 2/5\n",
      "6954/6954 [==============================] - 1523s 219ms/step - loss: 0.0888 - accuracy: 0.9735 - val_loss: 0.0437 - val_accuracy: 0.9848\n",
      "Epoch 3/5\n",
      "6954/6954 [==============================] - 1549s 223ms/step - loss: 0.0652 - accuracy: 0.9820 - val_loss: 0.0179 - val_accuracy: 0.9945\n",
      "Epoch 4/5\n",
      "6954/6954 [==============================] - 1389s 200ms/step - loss: 0.0480 - accuracy: 0.9867 - val_loss: 0.0169 - val_accuracy: 0.9942\n",
      "Epoch 5/5\n",
      "6954/6954 [==============================] - 1223s 176ms/step - loss: 0.0449 - accuracy: 0.9882 - val_loss: 0.0101 - val_accuracy: 0.9969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dda62e85b0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79dd401a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mobile_net_1111.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80841c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d1da23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mobile_net_1112.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72402f30",
   "metadata": {},
   "source": [
    "## Test set으로 check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1f158bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"my_model_kind_1108.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "050821b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f04a11bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1257/1257 [08:42<00:00,  2.41it/s]\n"
     ]
    }
   ],
   "source": [
    "for b_x, b_y in tqdm(test_ds):\n",
    "    pred.extend(model.predict(b_x).tolist())\n",
    "    y.extend(b_y.numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dfb1e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.array(pred)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68af498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_one = np.array([np.argmax(x) for x in pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98c2bd86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9230769230769231"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pred_one == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5388bd60",
   "metadata": {},
   "source": [
    "## 사진 데이터 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6fef9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_preprocess(img_loc):\n",
    "    img = cv2.imread(img_loc)\n",
    "    img = img[:,:,::-1]  # bgr to rgb\n",
    "    img = cv2.resize(img, dsize=(224, 224), interpolation=cv2.INTER_AREA)\n",
    "#     img = img / 255\n",
    "    img = img.reshape(1, 224, 224, 3)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "62210442",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_loc = 'C:/Users/multicampus/projects/fresh/ap3.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4fa1d615",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img_preprocess(img_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6bd44eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ea16b2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple_yanggwang\n"
     ]
    }
   ],
   "source": [
    "print(cls[p[0].argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f7d89738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>apple_yanggwang</th>\n",
       "      <td>6.497335e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apple_fuji</th>\n",
       "      <td>2.159170e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>onion_red</th>\n",
       "      <td>4.401521e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>persimmon_bansi</th>\n",
       "      <td>1.507500e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mandarine_hallabong</th>\n",
       "      <td>1.283997e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>onion_white</th>\n",
       "      <td>1.190384e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pear_singo</th>\n",
       "      <td>9.025316e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>potato_sumi</th>\n",
       "      <td>8.397702e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>persimmon_daebong</th>\n",
       "      <td>6.385315e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mandarine_onjumilgam</th>\n",
       "      <td>5.824568e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chinese-cabbage</th>\n",
       "      <td>5.549147e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cabbage_red</th>\n",
       "      <td>4.766235e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>persimmon_booyu</th>\n",
       "      <td>4.538378e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>potato_seolbong</th>\n",
       "      <td>4.244656e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cabbage_green</th>\n",
       "      <td>1.006841e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radish_winter-radish</th>\n",
       "      <td>4.883904e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>garlic_uiseong</th>\n",
       "      <td>2.889896e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pear_chuhwang</th>\n",
       "      <td>3.793209e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 0\n",
       "apple_yanggwang       6.497335e-01\n",
       "apple_fuji            2.159170e-01\n",
       "onion_red             4.401521e-02\n",
       "persimmon_bansi       1.507500e-02\n",
       "mandarine_hallabong   1.283997e-02\n",
       "onion_white           1.190384e-02\n",
       "pear_singo            9.025316e-03\n",
       "potato_sumi           8.397702e-03\n",
       "persimmon_daebong     6.385315e-03\n",
       "mandarine_onjumilgam  5.824568e-03\n",
       "chinese-cabbage       5.549147e-03\n",
       "cabbage_red           4.766235e-03\n",
       "persimmon_booyu       4.538378e-03\n",
       "potato_seolbong       4.244656e-03\n",
       "cabbage_green         1.006841e-03\n",
       "radish_winter-radish  4.883904e-04\n",
       "garlic_uiseong        2.889896e-04\n",
       "pear_chuhwang         3.793209e-09"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(p[0], cls).sort_values(0, ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7f9767cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.3980662e-10, 2.1340211e-05, 7.6893708e-12, 4.0260730e-28,\n",
       "        7.8801871e-17, 5.9421174e-29, 2.4851473e-12, 3.4651566e-05,\n",
       "        5.7651682e-21, 2.0160280e-10, 1.6331439e-24, 4.5502165e-20,\n",
       "        6.7753440e-01, 1.0686379e-02, 3.1172329e-01, 1.6961783e-27,\n",
       "        1.5388557e-25, 5.5638545e-13]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2433f1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
